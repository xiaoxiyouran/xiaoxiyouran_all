<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xml:lang=
"en-US">
<head>
<title>Chapter 2: Overview of the Sampled Package</title>
<link rel="stylesheet" type="text/css" href="../../../../technotes/css/guide.css" />
</head>
<body>
<!-- STATIC HEADER -->

<!-- header start -->
<div id="javaseheader">
<div id="javaseheaderlogo">
<img src="../../../../images/javalogo.gif"
alt="Java logo" />
</div>
<div id="javaseheaderindex">

<a href=
"../../../../index.html">Documentation Contents</a>
</div>
<div class="clear"></div>
</div>

<!-- header end -->


<p><small><a href="contents.html">&lt; Contents</a></small></p>
<h1>Chapter 2: Overview of the Sampled Package</h1>
<p>&nbsp;</p>
<table width="80%" summary="" border="1" align="center"
cellpadding="5" cellspacing="0">
<tr>
<td>
<h2><u>Notes</u></h2>
<ol>
<li>Direct Audio mixers are always the default on Windows. They are
the default on Solaris if the Solaris audio mixer is enabled (see
Solaris man page for mixer). On Linux, they are the default only if
there is a device that supports mixing.</li>
<li>Because they are &quot;direct,&quot; Direct Audio mixers do not
support <code>SAMPLE_RATE</code> control.</li>
<li>The default mixer can be selected via the
<code>sound.properties</code> file.<br /></li>
</ol>
</td>
</tr>
</table>
<br />
<p><a name="a112322" id="a112322"></a> This chapter provides an
introduction to the Java Sound API's digital audio architecture,
which is accessed through the <code>javax.sound.sampled</code>
package. First, an explanation is given of the package's focus:
playback and capture of formatted audio data. Then this chapter
describes the three fundamental components required for playback or
capture: an audio data format, a line, and a mixer.
Th<strong>e</strong> <code>Line</code> interface and its
subinterfaces are introduced briefly.</p>
<a name="a112324" id="a112324"></a>
<h2>Design Goals</h2>
<p><a name="a112326" id="a112326"></a> Before examining the
elements of the Java Sound API, it helps to understand the
orientation of the <code>javax.sound.sampled</code> package.</p>
<a name="a112328" id="a112328"></a>
<h3>A Focus on Data Transport</h3>
<p><a name="a112330" id="a112330"></a> The
<code>javax.sound.sampled</code> package is fundamentally concerned
with audio transport&mdash;in other words, the Java Sound API
focuses on playback and capture. The central task that the Java
Sound API addresses is how to move bytes of formatted audio data
into and out of the system. This task involves opening audio input
and output devices and managing buffers that get filled with
real-time sound data. It can also involve mixing multiple streams
of audio into one stream (whether for input or output). The
transport of sound into or out of the system has to be correctly
handled when the user requests that the flow of sound be started,
paused, resumed, or stopped.</p>
<p><a name="a112332" id="a112332"></a> To support this focus on
basic audio input and output, the Java Sound API provides methods
for converting between various audio data formats, and for reading
and writing common types of sound files. However, it does not
attempt to be a comprehensive sound-file toolkit. A particular
implementation of the Java Sound API need not support an extensive
set of file types or data format conversions. Third-party service
providers can supply modules that "plug in" to an existing
implementation to support additional file types and
conversions.</p>
<a name="a112334" id="a112334"></a>
<h3>Buffered and Unbuffered Handling of Audio</h3>
<p><a name="a112336" id="a112336"></a> The Java Sound API can
handle audio transport in both a streaming, buffered fashion and an
in-memory, unbuffered fashion. "Streaming" is used here in a
general sense to refer to real-time handling of audio bytes; it
does not refer to the specific, well-known case of sending audio
over the Internet in a certain format. In other words, a stream of
audio is simply a continuous set of audio bytes that arrive more or
less at the same rate that they are to be handled (played,
recorded, etc.). Operations on the bytes commence before all the
data has arrived. In the streaming model, particularly in the case
of audio input rather than audio output, you do not necessarily
know in advance how long the sound is and when it will finish
arriving. You simply handle one buffer of audio data at a time,
until the operation is halted. In the case of audio output
(playback), you also need to buffer data if the sound you want to
play is too large to fit in memory all at once. In other words, you
deliver your audio bytes to the sound engine in chunks, and it
takes care of playing each sample at the right time. Mechanisms are
provided that make it easy to know how much data to deliver in each
chunk.</p>
<p><a name="a112339" id="a112339"></a> The Java Sound API also
permits unbuffered transport in the case of playback only, assuming
you already have all the audio data at hand and it is not too large
to fit in memory. In this situation, there is no need for the
application program to buffer the audio, although the buffered,
real-time approach is still available if desired. Instead, the
entire sound can be preloaded at once into memory for subsequent
playback. Because all the sound data is loaded in advance, playback
can start immediately, for example, as soon as the user clicks a
Start button. This can be an advantage compared to the buffered
model, where the playback has to wait for the first buffer to fill.
In addition, the in-memory, unbuffered model allows sounds to be
easily looped (cycled) or set to arbitrary positions in the
data.</p>
<p><a name="a112341" id="a112341"></a> These two models for
playback are discussed further in Chapter 4, "<a href=
"chapter4.html">Playing Back Audio</a>." Buffered recording is
discussed in Chapter 5, "<a href="chapter5.html">Capturing
Audio</a>."</p>
<a name="a112344" id="a112344"></a>
<h2>The Essentials: Format, Mixer, and Line</h2>
<p><a name="a112346" id="a112346"></a> To play or capture sound
using the Java Sound API, you need at least three things: formatted
audio data, a mixer, and a line. Each of these is explained
below.</p>
<a name="a112348" id="a112348"></a>
<h3>What Is Formatted Audio Data?</h3>
<p><a name="a112350" id="a112350"></a> Formatted audio data refers
to sound in any of a number of standard formats. The Java Sound API
distinguishes between <em>data formats</em> and <em>file
formats</em>.</p>
<a name="a112352" id="a112352"></a>
<h3>Data Formats</h3>
<p><a name="a112354" id="a112354"></a> A data format tells you how
to interpret a series of bytes of "raw" sampled audio data, such as
samples that have already been read from a sound file, or samples
that have been captured from the microphone input. You might need
to know, for example, how many bits constitute one sample (the
representation of the shortest instant of sound), and similarly you
might need to know the sound's sample rate (how fast the samples
are supposed to follow one another). When setting up for playback
or capture, you specify the data format of the sound you are
capturing or playing.</p>
<p><a name="a112356" id="a112356"></a> In the Java Sound API, a
data format is represented by an <code>AudioFormat</code> object,
which includes the following attributes:</p>
<ul>
<li style="list-style: none"><a name="a112358" id=
"a112358"></a></li>
<li>Encoding technique, usually pulse code modulation (PCM)
<a name="a112359" id="a112359"></a></li>
<li>Number of channels (1 for mono, 2 for stereo, etc.) <a name=
"a112360" id="a112360"></a></li>
<li>Sample rate (number of samples per second, per channel)
<a name="a112361" id="a112361"></a></li>
<li>Number of bits per sample (per channel) <a name="a112362" id=
"a112362"></a></li>
<li>Frame rate <a name="a112363" id="a112363"></a></li>
<li>Frame size in bytes <a name="a112364" id="a112364"></a></li>
<li>Byte order (big-endian or little-endian)
<p><a name="a114719" id="a114719"></a></p>
</li>
</ul>
PCM is one kind of encoding of the sound waveform. The Java Sound
API includes two PCM encodings that use linear quantization of
amplitude, and signed or unsigned integer values. Linear
quantization means that the number stored in each sample is
directly proportional (except for any distortion) to the original
sound pressure at that instant and similarly proportional to the
displacement of a loudspeaker or eardrum that is vibrating with the
sound at that instant. Compact discs, for example, use linear
PCM-encoded sound. Mu-law encoding and a-law encoding are common
nonlinear encodings that provide a more compressed version of the
audio data; these encodings are typically used for telephony or
recordings of speech. A nonlinear encoding maps the original
sound's amplitude to the stored value using a nonlinear function,
which can be designed to give more amplitude resolution to quiet
sounds than to loud sounds.
<p><a name="a112368" id="a112368"></a> A frame contains the data
for all channels at a particular time. For PCM-encoded data, the
frame is simply the set of simultaneous samples in all channels,
for a given instant in time, without any additional information. In
this case, the frame rate is equal to the sample rate, and the
frame size in bytes is the number of channels multiplied by the
sample size in bits, divided by the number of bits in a byte.</p>
<p><a name="a112370" id="a112370"></a> For other kinds of
encodings, a frame might contain additional information besides the
samples, and the frame rate might be completely different from the
sample rate. For example, consider the MP3 (MPEG-1 Audio Layer 3)
encoding, which is not explicitly mentioned in the current version
of the Java Sound API, but which could be supported by an
implementation of the Java Sound API or by a third-party service
provider. In MP3, each frame contains a bundle of compressed data
for a series of samples, not just one sample per channel. Because
each frame encapsulates a whole series of samples, the frame rate
is slower than the sample rate. The frame also contains a header.
Despite the header, the frame size in bytes is less than the size
in bytes of the equivalent number of PCM frames. (After all, the
purpose of MP3 is to be more compact than PCM data.) For such an
encoding, the sample rate and sample size refer to the PCM data
that the encoded sound will eventually be converted into before
being delivered to a digital-to-analog converter (DAC).</p>
<a name="a112373" id="a112373"></a>
<h3>File Formats</h3>
<p><a name="a112375" id="a112375"></a> A file format specifies the
structure of a sound file, including not only the format of the raw
audio data in the file, but also other information that can be
stored in the file. Sound files come in various standard varieties,
such as WAVE (also known as WAV, and often associated with PCs),
AIFF (often associated with Macintoshes), and AU (often associated
with UNIX systems). The different types of sound file have
different structures. For example, they might have a different
arrangement of data in the file's "header." A header contains
descriptive information that typically precedes the file's actual
audio samples, although some file formats allow successive "chunks"
of descriptive and audio data. The header includes a specification
of the data format that was used for storing the audio in the sound
file. Any of these types of sound file can contain various data
formats (although usually there is only one data format within a
given file), and the same data format can be used in files that
have different file formats.</p>
<p><a name="a112377" id="a112377"></a> In the Java Sound API, a
file format is represented by an <code>AudioFileFormat</code>
object, which contains:</p>
<ul>
<li style="list-style: none"><a name="a112379" id=
"a112379"></a></li>
<li>The file type (WAVE, AIFF, etc.) <a name="a112380" id=
"a112380"></a></li>
<li>The file's length in bytes <a name="a112381" id=
"a112381"></a></li>
<li>The length, in frames, of the audio data contained in the file
<a name="a112382" id="a112382"></a></li>
<li>An AudioFormat object that specifies the data format of the
audio data contained in the file
<p><a name="a112383" id="a112383"></a></p>
</li>
</ul>
The <code>AudioSystem</code> class (described in Chapter 3,
"<a href="chapter3.html">Accessing Audio System Resources</a>")
provides methods for reading and writing sounds in different file
formats, and for converting between different data formats. Some of
the methods let you access a file's contents through a kind of
stream called an <code>AudioInputStream</code>. An
<code>AudioInputStream</code> is a subclass of the generic Java
<code>InputStream</code> class, which encapsulates a series of
bytes that can be read sequentially. To its superclass, the
<code>AudioInputStream</code> class adds knowledge of the bytes'
audio data format (represented by an <code>AudioFormat</code>
object). By reading a sound file as an
<code>AudioInputStream</code>, you get immediate access to the
samples, without having to worry about the sound file's structure
(its header, chunks, etc.). A single method invocation gives you
all the information you need about the data format and the file
type. <a name="a112387" id="a112387"></a>
<h3>What Is a Mixer?</h3>
<p><a name="a112389" id="a112389"></a> Many application programming
interfaces (APIs) for sound make use of the notion of an audio
<em>device</em>. A device is often a software interface to a
physical input/output device. For example, a sound-input device
might represent the input capabilities of a sound card, including a
microphone input, a line-level analog input, and perhaps a digital
audio input.</p>
<p><a name="a112391" id="a112391"></a> In the Java Sound API,
devices are represented by <code>Mixer</code> objects. The purpose
of a mixer is to handle one or more streams of audio input and one
or more streams of audio output. In the typical case, it actually
mixes together multiple incoming streams into one outgoing stream.
A <code>Mixer</code> object can represent the sound-mixing
capabilities of a physical device such as a sound card, which might
need to mix the sound coming in to the computer from various
inputs, or the sound coming from application programs and going to
outputs.</p>
<p><a name="a113681" id="a113681"></a> Alternatively, a
<code>Mixer</code> object can represent sound-mixing capabilities
that are implemented entirely in software, without any inherent
interface to physical devices.</p>
<p><a name="a112393" id="a112393"></a> In the Java Sound API, a
component such as the microphone input on a sound card is not
itself considered a device&mdash;that is, a mixer&mdash;but rather
a <em>port</em> into or out of the mixer. A port typically provides
a single stream of audio into or out of the mixer (although the
stream can be multichannel, such as stereo). The mixer might have
several such ports. For example, a mixer representing a sound
card's output capabilities might mix several streams of audio
together, and then send the mixed signal to any or all of various
output ports connected to the mixer. These output ports could be
(for example) a headphone jack, a built-in speaker, or a line-level
output.</p>
<p><a name="a112395" id="a112395"></a> To understand the notion of
a mixer in the Java Sound API, it helps to visualize a physical
mixing console, such as those used in live concerts and recording
studios. (See illustration that follows.)</p>
<p><a name="a112397" id="a112397"></a> <img alt=
"Physical mixing console" src="images/chapter2.anc1.gif" width=
"485" height="390" /></p>
<i>A Physical Mixing Console</i>
<p><a name="a113922" id="a113922"></a> A physical mixer has
"strips" (also called "slices"), each representing a path through
which a single audio signal goes into the mixer for processing. The
strip has knobs and other controls by which you can control the
volume and pan (placement in the stereo image) for the signal in
that strip. Also, the mixer might have a separate bus for effects
such as reverb, and this bus can be connected to an internal or
external reverberation unit. Each strip has a potentiometer that
controls how much of that strip's signal goes into the reverberated
mix. The reverberated ("wet") mix is then mixed with the "dry"
signals from the strips. A physical mixer sends this final mixture
to an output bus, which typically goes to a tape recorder (or
disk-based recording system) and/or speakers.</p>
<p><a name="a112401" id="a112401"></a> Imagine a live concert that
is being recorded in stereo. Cables (or wireless connections)
coming from the many microphones and electric instruments on stage
are plugged into the inputs of the mixing console. Each input goes
to a separate strip of the mixer, as illustrated. The sound
engineer decides on the settings of the gain, pan, and reverb
controls. The output of all the strips and the reverb unit are
mixed together into two channels. These two channels go to two
outputs on the mixer, into which cables are plugged that connect to
the stereo tape recorder's inputs. The two channels are perhaps
also sent via an amplifier to speakers in the hall, depending on
the type of music and the size of the hall.</p>
<p><a name="a112403" id="a112403"></a> Now imagine a recording
studio, in which each instrument or singer is recorded to a
separate track of a multitrack tape recorder. After the instruments
and singers have all been recorded, the recording engineer performs
a "mixdown" to combine all the taped tracks into a two-channel
(stereo) recording that can be distributed on compact discs. In
this case, the input to each of the mixer's strips is not a
microphone, but one track of the multitrack recording. Once again,
the engineer can use controls on the strips to decide each track's
volume, pan, and reverb amount. The mixer's outputs go once again
to a stereo recorder and to stereo speakers, as in the example of
the live concert.</p>
<p><a name="a112817" id="a112817"></a> These two examples
illustrate two different uses of a mixer: to capture multiple input
channels, combine them into fewer tracks, and save the mixture, or
to play back multiple tracks while mixing them down to fewer
tracks.</p>
<p><a name="a112407" id="a112407"></a> In the Java Sound API, a
mixer can similarly be used for input (capturing audio) or output
(playing back audio). In the case of input, the <em>source</em>
from which the mixer gets audio for mixing is one or more input
ports. The mixer sends the captured and mixed audio streams to its
<em>target</em>, which is an object with a buffer from which an
application program can retrieve this mixed audio data. In the case
of audio output, the situation is reversed. The mixer's source for
audio is one or more objects containing buffers into which one or
more application programs write their sound data; and the mixer's
target is one or more output ports.</p>
<a name="a113943" id="a113943"></a>
<h3>What Is a Line?</h3>
<p><a name="a113931" id="a113931"></a> The metaphor of a physical
mixing console is also useful for understanding the Java Sound
API's concept of a <em>line</em>.</p>
<p><a name="a113932" id="a113932"></a> A line is an element of the
digital audio "pipeline"&mdash;that is, a path for moving audio
into or out of the system. Usually the line is a path into or out
of a mixer (although technically the mixer itself is also a kind of
line).</p>
<p><a name="a112416" id="a112416"></a> Audio input and output ports
are lines. These are analogous to the microphones and speakers
connected to a physical mixing console. Another kind of line is a
data path through which an application program can get input audio
from, or send output audio to, a mixer. These data paths are
analogous to the tracks of the multitrack recorder connected to the
physical mixing console.</p>
<p><a name="a112418" id="a112418"></a> One difference between lines
in the Java Sound API and those of a physical mixer is that the
audio data flowing through a line in the Java Sound API can be mono
or multichannel (for example, stereo). By contrast, each of a
physical mixer's inputs and outputs is typically a single channel
of sound. To get two or more channels of output from the physical
mixer, two or more physical outputs are normally used (at least in
the case of analog sound; a digital output jack is often
multichannel). In the Java Sound API, the number of channels in a
line is specified by the <code>AudioFormat</code> of the data that
is currently flowing through the line.</p>
<a name="a112421" id="a112421"></a>
<h3>Lines in an Audio-output Configuration</h3>
<p><a name="a112423" id="a112423"></a> Let's now examine some
specific kinds of lines and mixers. The following diagram shows
different types of lines in a simple audio-output system that could
be part of an implementation of the Java Sound API:</p>
<p><a name="a112427" id="a112427"></a> <img alt=
"Possible configuration of lines for audio output" src=
"images/chapter2.anc.gif" width="380" height="100" /></p>
<i>A Possible Configuration of Lines for Audio Output</i>
<p><a name="a112429" id="a112429"></a> In this example, an
application program has gotten access to some available inputs of
an audio-input mixer: one or more <em>clips</em> and <em>source
data lines</em>. A clip is a mixer input (a kind of line) into
which you can load audio data prior to playback; a source data line
is a mixer input that accepts a real-time stream of audio data. The
application program preloads audio data from a sound file into the
clips. It then pushes other audio data into the source data lines,
a buffer at a time. The mixer reads data from all these lines, each
of which may have its own reverberation, gain, and pan controls,
and mixes the dry audio signals with the wet (reverberated) mix.
The mixer delivers its final output to one or more output ports,
such as a speaker, a headphone jack, and a line-out jack.</p>
<p><a name="a112431" id="a112431"></a> Although the various lines
are depicted as separate rectangles in the diagram, they are all
"owned" by the mixer, and can be considered integral parts of the
mixer. The reverb, gain, and pan rectangles represent processing
controls (rather than lines) that can be applied by the mixer to
data flowing through the lines.</p>
<p><a name="a112433" id="a112433"></a> Note that this is just one
example of a possible mixer that is supported by the API. Not all
audio configurations will have all the features illustrated. An
individual source data line might not support panning, a mixer
might not implement reverb, and so on.</p>
<a name="a112435" id="a112435"></a>
<h3>Lines in an Audio-input Configuration</h3>
<p><a name="a112437" id="a112437"></a> A simple audio-input system
might be similar:</p>
<p><a name="a112439" id="a112439"></a> <img alt=
"Possible configuration of lines for audio input" src=
"images/chapter2.anc2.gif" width="291" height="90" /></p>
<i>A Possible Configuration of Lines for Audio Input</i>
<p><a name="a112860" id="a112860"></a> Here, data flows into the
mixer from one or more input ports, commonly the microphone or the
line-in jack. Gain and pan are applied, and the mixer delivers the
captured data to an application program via the mixer's target data
line. A target data line is a mixer output, containing the mixture
of the streamed input sounds. The simplest mixer has just one
target data line, but some mixers can deliver captured data to
multiple target data lines simultaneously.</p>
<a name="a112445" id="a112445"></a>
<h3>The Line Interface Hierarchy</h3>
<p><a name="a112447" id="a112447"></a> Now that we've seen some
functional pictures of what lines and mixers are, let's discuss
them from a slightly more programmatic perspective. Several types
of line are defined by subinterfaces of the basic <code>Line</code>
interface. The interface hierarchy is shown below.</p>
<p><a name="a112449" id="a112449"></a> <img alt=
"Line interface hierarchy" src="images/chapter2.anc3.gif" width=
"402" height="209" /></p>
<i>The Line Interface Hierarchy</i>
<p><a name="a112452" id="a112452"></a> The base interface,
<code>Line</code>, describes the minimal functionality common to
all lines:</p>
<ul>
<li style="list-style: none"><a name="a112454" id=
"a112454"></a></li>
<li>Controls <a name="a112456" id="a112456"></a> Data lines and
ports often have a set of controls that affect the audio signal
passing through the line. The Java Sound API specifies control
classes that can be used to manipulate aspects of sound such as:
gain (which affects the signal's volume in decibels), pan (which
affects the sound's right-left positioning, reverb (which adds
reverberation to the sound to emulate different kinds of room
acoustics), and sample rate (which affects the rate of playback as
well as the sound's pitch). <a name="a112458" id=
"a112458"></a></li>
<li>Open or closed status <a name="a112460" id="a112460"></a>
Successful opening of a line guarantees that resources have been
allocated to the line. A mixer has a finite number of lines, so at
some point multiple application programs (or the same one) might
vie for usage of the mixer's lines. Closing a line indicates that
any resources used by the line may now be released. <a name=
"a112462" id="a112462"></a></li>
<li>Events <a name="a112464" id="a112464"></a> A line generates
events when it opens or closes. Subinterfaces of <code>Line</code>
can introduce other types of events. When a line generates an
event, the event is sent to all objects that have registered to
"listen" for events on that line. An application program can create
these objects, register them to listen for line events, and react
to the events as desired.
<p><a name="a112466" id="a112466"></a></p>
</li>
</ul>
We'll now examine the subinterfaces of the <code>Line</code>
interface.
<p><a name="a112468" id="a112468"></a> <code>Ports</code> are
simple lines for input or output of audio to or from audio devices.
As mentioned earlier, some common types of ports are the
microphone, line input, CD-ROM drive, speaker, headphone, and line
output.</p>
<p><a name="a112470" id="a112470"></a> The <code>Mixer</code>
interface represents a mixer, of course, which as we have seen
represents either a hardware or a software device. The
<code>Mixer</code> interface provides methods for obtaining a
mixer's lines. These include source lines, which feed audio to the
mixer, and target lines, to which the mixer delivers its mixed
audio. For an audio-input mixer, the source lines are input ports
such as the microphone input, and the target lines are
<code>TargetDataLines</code> (described below), which deliver audio
to the application program. For an audio-output mixer, on the other
hand, the source lines are <code>Clips</code> or
<code>SourceDataLines</code> (described below), to which the
application program feeds audio data, and the target lines are
output ports such as the speaker.</p>
<p><a name="a112472" id="a112472"></a> A <code>Mixer</code> is
defined as having one or more source lines and one or more target
lines. Note that this definition means that a mixer need not
actually mix data; it might have only a single source line. The
<code>Mixer</code> API is intended to encompass a variety of
devices, but the typical case supports mixing.</p>
<p><a name="a112474" id="a112474"></a> The <code>Mixer</code>
interface supports synchronization; that is, you can specify that
two or more of a mixer's lines be treated as a synchronized group.
Then you can start, stop, or close all those data lines by sending
a single message to any line in the group, instead of having to
control each line individually. With a mixer that supports this
feature, you can obtain sample-accurate synchronization between
lines.</p>
<p><a name="a112476" id="a112476"></a> The generic
<code>Line</code> interface does not provide a means to start and
stop playback or recording. For that you need a data line. The
<code>DataLine</code> interface supplies the following additional
media-related features beyond those of a <code>Line</code>:</p>
<a name="a112477" id="a112477"></a>
<ul>
<li>Audio format <a name="a112479" id="a112479"></a> Each data line
has an audio format associated with its data stream. <a name=
"a112987" id="a112987"></a></li>
<li>Media position <a name="a112482" id="a112482"></a> A data line
can report its current position in the media, expressed in sample
frames. This represents the number of sample frames captured by or
rendered from the data line since it was opened. <a name="a112483"
id="a112483"></a></li>
<li>Buffer size <a name="a112485" id="a112485"></a> This is the
size of the data line's internal buffer in bytes. For a source data
line, the internal buffer is one to which data can be written, and
for a target data line it's one from which data can be read.
<a name="a112486" id="a112486"></a></li>
<li>Level (the current amplitude of the audio signal)
<p><a name="a112487" id="a112487"></a></p>
</li>
<li>Start and stop playback or capture
<p><a name="a112488" id="a112488"></a></p>
</li>
<li>Pause and resume playback or capture
<p><a name="a112489" id="a112489"></a></p>
</li>
<li>Flush (discard unprocessed data from the queue)
<p><a name="a112490" id="a112490"></a></p>
</li>
<li>Drain (block until all unprocessed data has been drained from
the queue, and the data line's buffer has become empty)
<p><a name="a112491" id="a112491"></a></p>
</li>
<li>Active status <a name="a112493" id="a112493"></a> A data line
is considered active if it is engaged in active presentation or
capture of audio data to or from a mixer. <a name="a113005" id=
"a113005"></a></li>
<li>Events <a name="a112496" id="a112496"></a> <code>START</code>
and <code>STOP</code> events are produced when active presentation
or capture of data from or to the data line starts or stops.</li>
</ul>
<p><a name="a112499" id="a112499"></a> A
<code>TargetDataLine</code> receives audio data from a mixer.
Commonly, the mixer has captured audio data from a port such as a
microphone; it might process or mix this captured audio before
placing the data in the target data line's buffer. The
<code>TargetDataLine</code> interface provides methods for reading
the data from the target data line's buffer and for determining how
much data is currently available for reading.</p>
<p><a name="a112501" id="a112501"></a> A
<code>SourceDataLine</code> receives audio data for playback. It
provides methods for writing data to the source data line's buffer
for playback, and for determining how much data the line is
prepared to receive without blocking.</p>
<p><a name="a112503" id="a112503"></a> A <code>Clip</code> is a
data line into which audio data can be loaded prior to playback.
Because the data is pre-loaded rather than streamed, the clip's
duration is known before playback, and you can choose any starting
position in the media. Clips can be looped, meaning that upon
playback, all the data between two specified loop points will
repeat a specified number of times, or indefinitely.</p>
<p><a name="a112505" id="a112505"></a> This chapter has introduced
most of the important interfaces and classes of the sampled-audio
API. Subsequent chapters show how you can access and use these
objects in your application program.</p>
<p>&nbsp;</p>

<!--  footer start -->
<div id="javasefooter">
<div class="hr">
<hr /></div>
<div id="javasecopyright">
<img id="oraclelogofooter" src=
"../../../../images/oraclelogo.gif" alt="Oracle and/or its affiliates"
border="0" width="100" height="29" name=
"oraclelogofooter" />

<a href="../../../../legal/cpyr.html">Copyright
&#169;</a> 1993, 2018, Oracle and/or its affiliates. All rights
reserved.</div>
<div id="javasecontactus">
<a href=
"http://docs.oracle.com/javase/feedback.html">Contact
Us</a>
</div>
</div>
<!-- footer end -->

<!-- STATIC FOOTER -->

</body>
</html>
