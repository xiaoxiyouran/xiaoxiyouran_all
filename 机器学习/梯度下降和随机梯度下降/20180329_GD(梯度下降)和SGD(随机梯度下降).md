# GD(梯度下降)和SGD(随机梯度下降)

# 相同点

在GD和SGD中，都会在每次迭代中更新模型的参数，使得代价函数变小。

# 不同点

## GD

在GD中，每次迭代都要用到**全部**训练数据。
假设线性模型

![img](http://upload-images.jianshu.io/upload_images/1825085-b125c8bcbfb0521e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/211)

Paste_Image.png

θ是参数

代价函数 ：

![img](http://upload-images.jianshu.io/upload_images/1825085-34f050910a43da4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/274)

Paste_Image.png

那么每次GD的更新算法为：

![img](http://upload-images.jianshu.io/upload_images/1825085-ffd35c111f2f6039.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/206)

Paste_Image.png

由此算法可知，在对代价函数求偏导时，是需要用到全部的训练数据的。

## SGD

在SGD中，每次迭代可以只用**一个**训练数据来更新参数。
回到GD的更新算法，假设此时我们此时训练数据就只有一条(x,y)，

![img](http://upload-images.jianshu.io/upload_images/1825085-aac6d25d490c72f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/432)

Paste_Image.png

所以此时的更新参数的算法变为：

![img](http://upload-images.jianshu.io/upload_images/1825085-a08b3af9b8250e20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/293)

Paste_Image.png

此时更新的算法，只用到了一个样本。
其实具象的理解下，就是来了一条训练数据，算下此时根据模型算出的值和实际值的差距，如果差距大，那么参数更新的幅度大，反之则小。

# 总结

当训练数据过大时，用GD可能造成内存不够用，那么就可以用SGD了，SGD其实可以算作是一种online-learning。另外SGD收敛会比GD快，但是对于代价函数求最小值还是GD做的比较好，不过SGD也够用了。